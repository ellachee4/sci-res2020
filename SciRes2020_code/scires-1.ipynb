{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data & Process\n",
    "\n",
    "1. Import MIT-BIH Arrhythmia ECG records and annotations\n",
    "2. Import MIT-BIH Atrial Fibrillation ECG records and annotations\n",
    "3. Resample all data to the same frequency (360Hz)\n",
    "4. Take each patient record and split it into 30 sec. samples\n",
    "    - 10 hr. records from AFIB ECGs -> 1200 samples (27600 samples from 23 total ECGs)\n",
    "    - .5 hr. records from Arrhythmia ECGs -> 60 samples (2880 samples from 48 total ECGs)\n",
    "    \n",
    "total ECG samples = 30480\n",
    "\n",
    "using 30420, 30s ECG samples (60 are bad) \n",
    "\n",
    "total only AFIB samples = 11234 (1% of dataset, close to real world proportion = .5%)\n",
    "\n",
    "total containing AFIB samples = 11296 = 37.13% of dataset\n",
    "\n",
    "total normal samples = 17888 = 58.80% of dataset\n",
    "\n",
    "other samples = 1236 = 4.06% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2820 arrhythmia pieces\n",
      "Loaded 2820 arrhythmia annotations\n",
      "Found 8 afib pieces\n",
      "Found 25 afib and other pieces\n",
      "Found 1048 pieces with other arrhythmia\n",
      "Found 1739 normal pieces\n",
      "Found 60 bad pieces\n"
     ]
    }
   ],
   "source": [
    "# arrhythmia ECG records\n",
    "\n",
    "import wfdb\n",
    "\n",
    "record_ids = [\n",
    "    '100','101','102','103','104','105','106','107','108',\n",
    "    '109','111','112','113','114','115','116','117','118',\n",
    "    '119','121','122','123','124','200','201','202','203',\n",
    "    '205','207','208','209','210','212','213','214','215',\n",
    "    '217','219','220','221','222','223','228','230','231',\n",
    "    '232','233','234']\n",
    "\n",
    "# records = []\n",
    "# for rec in record_ids:\n",
    "#     record = wfdb.rdrecord(rec, pn_dir='mitdb')\n",
    "#     records.append(record)\n",
    "\n",
    "# to cut arrhythmia pieces\n",
    "num_samps = 10800\n",
    "arrhythmia_pieces = []\n",
    "arrhythmia_ann_pieces = []\n",
    "arrhythmia_afib_pieces = []\n",
    "arrhythmia_normal_pieces = []\n",
    "arrhythmia_other_pieces = []\n",
    "arrhythmia_afib_and_other_pieces = []\n",
    "arrhythmia_bad_pieces = 0\n",
    "\n",
    "bad_ann = { 'qq\\x00', 'U\\x00', 'M\\x00', 'MISSB\\x00', 'P\\x00', 'PE\\x00', 'T\\x00', 'TS\\x00' }\n",
    "\n",
    "for rec_id in record_ids:\n",
    "    # read rec and ann\n",
    "    rec = wfdb.rdrecord('mit-bih-arrhythmia-database-1.0.0/'+rec_id)\n",
    "    ann = wfdb.rdann('mit-bih-arrhythmia-database-1.0.0/'+rec_id, extension='atr')\n",
    "    start = 0\n",
    "    end = num_samps\n",
    "    start_ann = None\n",
    "    while end <= 648000:\n",
    "        # create the annotations array for the 30 second piece\n",
    "        arrhythmia_ann_piece = []\n",
    "        has_afib = (start_ann == '(AFIB')\n",
    "        has_other = (start_ann not in bad_ann and start_ann != '(AFIB' and start_ann != '(N\\x00')\n",
    "        is_bad = (start_ann in bad_ann)\n",
    "        if start_ann != None:\n",
    "            arrhythmia_ann_piece.append([0, start_ann])\n",
    "        for i in range(0, ann.__dict__['ann_len']):\n",
    "            ann_symbol = ann.__dict__['aux_note'][i]\n",
    "            if start <= ann.__dict__['sample'][i] and ann.__dict__['sample'][i] < end and ann_symbol != '':\n",
    "                arrhythmia_ann_piece.append([\n",
    "                    ann.__dict__['sample'][i] - start,\n",
    "                    ann_symbol\n",
    "                ])\n",
    "                # find bad pieces\n",
    "                if ann_symbol in bad_ann:\n",
    "                    is_bad = True\n",
    "                # find pieces with afib anns\n",
    "                elif ann_symbol == '(AFIB':\n",
    "                    has_afib = True\n",
    "                # find pieces with other anns\n",
    "                elif ann_symbol != '(AFIB' and ann_symbol != '(N\\x00':\n",
    "                    has_other = True\n",
    "                start_ann = ann_symbol\n",
    "        # identify afib pieces\n",
    "        piece_index = len(arrhythmia_ann_pieces)\n",
    "        # identify bad pieces\n",
    "        if is_bad:\n",
    "            arrhythmia_bad_pieces += 1\n",
    "        else:\n",
    "            # cut out the 30 second piece\n",
    "            arrhythmia_pieces.append(rec.__dict__['p_signal'][start:end])\n",
    "            arrhythmia_ann_pieces.append(arrhythmia_ann_piece)\n",
    "            # identify afib pieces\n",
    "            if has_afib:\n",
    "                # identify afib and other pieces\n",
    "                if has_other:\n",
    "                    arrhythmia_afib_and_other_pieces.append(piece_index)\n",
    "                # identify just afib pieces\n",
    "                else:\n",
    "                    arrhythmia_afib_pieces.append(piece_index)\n",
    "            # identify no afib pieces\n",
    "            else:\n",
    "                # identify other arrhythmia pieces\n",
    "                if has_other:\n",
    "                    arrhythmia_other_pieces.append(piece_index)\n",
    "                # identify normal pieces\n",
    "                else:\n",
    "                    arrhythmia_normal_pieces.append(piece_index)\n",
    "        start = end\n",
    "        end += num_samps\n",
    "        \n",
    "print('Loaded',len(arrhythmia_pieces),'arrhythmia pieces')\n",
    "print('Loaded',len(arrhythmia_ann_pieces),'arrhythmia annotations')\n",
    "print('Found',len(arrhythmia_afib_pieces),'afib pieces')\n",
    "print('Found',len(arrhythmia_afib_and_other_pieces),'afib and other pieces')\n",
    "print('Found',len(arrhythmia_other_pieces),'pieces with other arrhythmia')\n",
    "print('Found',len(arrhythmia_normal_pieces),'normal pieces')\n",
    "print('Found',arrhythmia_bad_pieces,'bad pieces')\n",
    "\n",
    "# x_record_ids = [\n",
    "#     'x_108','x_109','x_111','x_112','x_113',\n",
    "#     'x_114','x_115','x_116','x_117','x_121',\n",
    "#     'x_122','x_123','x_124','x_220','x_221',\n",
    "#     'x_222','x_223','x_228','x_230','x_231',\n",
    "#     'x_232','x_233','x_234']\n",
    "# x_annotations = []\n",
    "# x_records = []\n",
    "# for rec in x_record_ids:\n",
    "#     record = wfdb.rdrecord(rec, pn_dir='mitdb/x_mitdb')\n",
    "#     x_records.append(record)\n",
    "#     annotation = wfdb.rdann(rec, pn_dir='mitdb/x_mitdb', extension='atr')\n",
    "#     x_annotations.append(annotation)\n",
    "# print('Loaded', len(x_records), 'x record(s)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27600 afib pieces\n",
      "Loaded 27600 afib annotations\n",
      "Found 11226 afib pieces\n",
      "Found 37 afib and other pieces\n",
      "Found 188 pieces with other arrhythmia\n",
      "Found 16149 normal pieces\n"
     ]
    }
   ],
   "source": [
    "# atrial fibrillation ECG records\n",
    "\n",
    "import wfdb.processing\n",
    "\n",
    "# load MIT BIH AFib data & annotations\n",
    "afib_record_ids = [\n",
    "#     '00735', '03665', \n",
    "    '04015', '04043', '04048', '04126', \n",
    "    '04746', '04908', '04936', '05091', '05121', '05261', \n",
    "    '06426', '06453', '06995', '07162', '07859', '07879', \n",
    "    '07910', '08215', '08219', '08378', '08405', '08434', \n",
    "    '08455']\n",
    "\n",
    "num_samps = 10800\n",
    "\n",
    "afib_pieces = []\n",
    "afib_ann_pieces = []\n",
    "afib_afib_pieces = []\n",
    "afib_normal_pieces = []\n",
    "afib_other_pieces = []\n",
    "afib_afib_and_other_pieces = []\n",
    "\n",
    "for rec_id in afib_record_ids:\n",
    "    # read rec and ann\n",
    "    rec = wfdb.rdrecord('mit-bih-atrial-fibrillation-database-1.0.0/'+rec_id)\n",
    "    ann = wfdb.rdann('mit-bih-atrial-fibrillation-database-1.0.0/'+rec_id, extension='atr')\n",
    "    # resample rec and ann to 360 Hz\n",
    "    afib_resampled, afib_annotation_resampled = wfdb.processing.resample_multichan(rec.__dict__['p_signal'], ann, 250, 360)\n",
    "    ann_samples = [int(round(i*360.0/250.0)) for i in ann.__dict__['sample']]\n",
    "    start = 0\n",
    "    end = num_samps\n",
    "    start_ann = None\n",
    "    while end <= 12960000:\n",
    "        # cut out the 30 second piece\n",
    "        afib_pieces.append(afib_resampled[start:end])\n",
    "        # create the annotations array for the 30 second piece\n",
    "        ann_piece = []\n",
    "        has_afib = (start_ann == '(AFIB')\n",
    "        has_other = (start_ann != '(AFIB' and start_ann != '(N')\n",
    "        if start_ann != None:\n",
    "            ann_piece.append([0, start_ann])\n",
    "        for i in range(0, ann.__dict__['ann_len']):\n",
    "            ann_symbol = ann.__dict__['aux_note'][i]\n",
    "            if start <= ann_samples[i] and ann_samples[i] < end:\n",
    "                ann_piece.append([\n",
    "                    ann_samples[i] - start,\n",
    "                    ann_symbol])\n",
    "                # find pieces with afib anns\n",
    "                if ann_symbol == '(AFIB':\n",
    "                    has_afib = True\n",
    "                # find pieces with other anns\n",
    "                elif ann_symbol != '(AFIB' and ann_symbol != '(N':\n",
    "                    has_other = True\n",
    "                start_ann = ann_symbol\n",
    "        # identify afib pieces\n",
    "        piece_index = len(afib_ann_pieces)\n",
    "        # identify afib pieces\n",
    "        if has_afib:\n",
    "            # identify afib and other pieces\n",
    "            if has_other:\n",
    "                afib_afib_and_other_pieces.append(piece_index)\n",
    "            # identify just afib pieces\n",
    "            else:\n",
    "                afib_afib_pieces.append(piece_index)\n",
    "        # identify no afib pieces\n",
    "        else:\n",
    "            # identify other arrhythmia pieces\n",
    "            if has_other:\n",
    "                afib_other_pieces.append(piece_index)\n",
    "            # identify normal pieces\n",
    "            else:\n",
    "                afib_normal_pieces.append(piece_index)\n",
    "        afib_ann_pieces.append(ann_piece)\n",
    "        start = end\n",
    "        end += num_samps\n",
    "        \n",
    "print('Loaded',len(afib_pieces),'afib pieces')\n",
    "print('Loaded',len(afib_ann_pieces),'afib annotations')\n",
    "print('Found',len(afib_afib_pieces),'afib pieces')\n",
    "print('Found',len(afib_afib_and_other_pieces),'afib and other pieces')\n",
    "print('Found',len(afib_other_pieces),'pieces with other arrhythmia')\n",
    "print('Found',len(afib_normal_pieces),'normal pieces')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 30s pieces from arrhythmia and afib databases\n",
    "all_pieces = arrhythmia_pieces + afib_pieces\n",
    "all_ann_pieces = arrhythmia_ann_pieces + afib_ann_pieces\n",
    "all_afib_pieces = arrhythmia_afib_pieces + [i + len(arrhythmia_pieces) for i in afib_afib_pieces]\n",
    "all_afib_and_other_pieces = arrhythmia_afib_and_other_pieces + [i + len(arrhythmia_pieces) for i in afib_afib_and_other_pieces]\n",
    "all_other_pieces = arrhythmia_other_pieces + [i + len(arrhythmia_pieces) for i in afib_other_pieces]\n",
    "all_normal_pieces = arrhythmia_normal_pieces + [i + len(arrhythmia_pieces) for i in afib_normal_pieces]\n",
    "\n",
    "# label all pieces: N = normal, A = afib, B = both afib and other, O = other\n",
    "all_pieces_labels = ['N' for i in all_pieces]\n",
    "for i in all_afib_pieces:\n",
    "    all_pieces_labels[i] = 'A'\n",
    "for i in all_afib_and_other_pieces:\n",
    "    all_pieces_labels[i] = 'B'\n",
    "for i in all_other_pieces:\n",
    "    all_pieces_labels[i] = 'O'\n",
    "\n",
    "# get target labels for output data (0 if not AFIB, 1 if AFIB)\n",
    "all_pieces_targets = [1 if (l == 'A' or l == 'B') else 0 for l in all_pieces_labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_pieces and all_pieces_targets to files\n",
    "\n",
    "import pickle\n",
    "\n",
    "all_pieces_file = open('all_pieces.pkl', mode='wb')\n",
    "pickle.dump(all_pieces, all_pieces_file)\n",
    "all_pieces_file.close()\n",
    "\n",
    "all_pieces_targets_file = open('all_pieces_targets.pkl', mode='wb')\n",
    "pickle.dump(all_pieces_targets, all_pieces_targets_file)\n",
    "all_pieces_targets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all_pieces and all_pieces_targets from files\n",
    "\n",
    "import pickle\n",
    "\n",
    "all_pieces_file = open('all_pieces.pkl', mode='rb')\n",
    "all_pieces = pickle.load(all_pieces_file)\n",
    "all_pieces_file.close()\n",
    "\n",
    "all_pieces_targets_file = open('all_pieces_targets.pkl', mode='rb')\n",
    "all_pieces_targets = pickle.load(all_pieces_targets_file)\n",
    "all_pieces_targets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 5-fold cross validation to get train and test sets\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "for train_set, test_set in kfold.split(all_pieces, all_pieces_targets):\n",
    "    train_sets.append(train_set)\n",
    "    test_sets.append(test_set)\n",
    "\n",
    "# train set 1\n",
    "train_set_pieces_raw = [all_pieces[i] for i in train_sets[0]]\n",
    "#input - train set 1 MLII signal only        \n",
    "train_set_pieces = [[s[0] for s in piece] for piece in train_set_pieces_raw]\n",
    "# output - target labels\n",
    "train_set_targets = [all_pieces_targets[i] for i in train_sets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train_set_targets shape:  (24336, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# one-hot encoding of train_set_targets\n",
    "train_set_targets_one_hot = np_utils.to_categorical(train_set_targets, 2)\n",
    "print('New train_set_targets shape: ', train_set_targets_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape, GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# 1D CNN to identify AFIB or not AFIB\n",
    "model_1 = Sequential()\n",
    "model_1.add(Conv1D(filters=100, kernel_size=18, activation='relu', input_shape=(len(train_set_pieces), 10800)))\n",
    "model_1.add(Conv1D(100, 5, activation='relu'))\n",
    "model_1.add(MaxPooling1D(3))\n",
    "model_1.add(Conv1D(160, 10, activation='relu'))\n",
    "model_1.add(Conv1D(160, 10, activation='relu'))\n",
    "model_1.add(GlobalAveragePooling1D())\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(activation='softmax', units=1))\n",
    "print(model_1.summary())\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_1.fit(train_set_pieces,\n",
    "                      train_set_targets_one_hot,\n",
    "                      batch_size=32,\n",
    "                      epochs=1,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
